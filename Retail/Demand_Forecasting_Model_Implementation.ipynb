{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose:** The reason for this notebook is to find the algorithm that gives the best forecast accuracy for SKU sales data.\n",
    "\n",
    "**Challenge**: Essentially, we want to be able to take SKU data, pass it through the model, and find out what is the best timeseries and ML algo for us to forecast with. \n",
    "\n",
    "**Business Goal:** We then take the winning timeseries and ML algo and apply it to the specific SKU in a business context. Periodically, we re-run the model when more data is available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Import Modules](#Import)\n",
    "    * [Section 1.1](#section_1_1)\n",
    "    * [Section 1.2](sSection_1_2)\n",
    "        * [Section 1.2.1](#section_1_2_1)\n",
    "        * [Section 1.2.2](#section_1_2_2)\n",
    "        * [Section 1.2.3](#section_1_2_3)\n",
    "* [Chapter 2](#chapter2)\n",
    "    * [Section 2.1](#section_2_1)\n",
    "    * [Section 2.2](#section_2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 1 <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "\n",
    "#### Section 1.1 <a class=\"anchor\" id=\"section_1_1\"></a>\n",
    "\n",
    "#### Section 1.2 <a class=\"anchor\" id=\"section_1_2\"></a>\n",
    "\n",
    "##### Section 1.2.1 <a class=\"anchor\" id=\"section_1_2_1\"></a>\n",
    "\n",
    "##### Section 1.2.2 <a class=\"anchor\" id=\"section_1_2_2\"></a>\n",
    "\n",
    "##### Section 1.2.3 <a class=\"anchor\" id=\"section_1_2_3\"></a>\n",
    "\n",
    "### Chapter 2 <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "\n",
    "#### Section 2.1 <a class=\"anchor\" id=\"section_2_1\"></a>\n",
    "\n",
    "#### Section 2.2 <a class=\"anchor\" id=\"section_2_2\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules <a class=\"anchor\" id=\"Import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import math\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from scipy.stats import randint as sp_randint\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input():\n",
    "    forecast_period = input(\"What is the Future Forecast period? \")\n",
    "    forecast_period = int(forecast_period)\n",
    "\n",
    "    input_data = input(\"Enter dataset: \")\n",
    "\n",
    "\n",
    "    file_type = os.path.splitext(input_data)[1]\n",
    "    if file_type == '.csv':\n",
    "        dataset = pd.read_csv(input_data)\n",
    "\n",
    "    elif file_type == '.json':\n",
    "        dataset = retrieve_data(input_data)\n",
    "\n",
    "    return forecast_period, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(path):\n",
    "    datasets = dict()\n",
    "    with open(path) as json_file:\n",
    "        raw_data = json.load(json_file)\n",
    "\n",
    "        for element in raw_data:\n",
    "            df_name = element['sku']\n",
    "            data = pd.DataFrame()\n",
    "            data['time'] = element['time']\n",
    "            data['sales'] = element['sales']\n",
    "            data['sales'] = [np.nan if pd.isnull(i) else int(i) for i in data['sales']]\n",
    "\n",
    "            data = data.T\n",
    "            data = data.rename(columns = data.iloc[0]).drop(data.index[0])\n",
    "            datasets[df_name] = copy.deepcopy(data)\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_standard_deviation(dataset):\n",
    "    mu = np.mean(dataset.values)\n",
    "    sd = np.std(dataset.values)\n",
    "\n",
    "    ub = mu + (3 * sd)\n",
    "    lb = mu - (3* sd)\n",
    "\n",
    "    return lb,ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dickeyfullertest(series):\n",
    "    dftest = adfuller(series, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index= ['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' % key] = value\n",
    "\n",
    "    if dfoutput['p-value'] > 0.05:\n",
    "        return 0 #not stationary\n",
    "\n",
    "    else:\n",
    "        return 1 #stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for autocorrelation function\n",
    "#\n",
    "#Autocorrelation measures the relationship \n",
    "#between a variable's current value and its past values.\n",
    "#\n",
    "\n",
    "def acf_plot1(dataset,freq):\n",
    "#    plot_acf(dataset)\n",
    "    res = acf(dataset)\n",
    "    ub=1.96/np.sqrt(len(dataset))\n",
    "    for i in range(1, len(res)-1):\n",
    "        if(res[i] > ub and res[i + 1] < ub):\n",
    "            p = i\n",
    "            if (p > freq):\n",
    "                p = freq\n",
    "            break\n",
    "    else:\n",
    "        p = freq\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way of plotting acf\n",
    "def acf_plot(dataset,freq):\n",
    "    res = acf(dataset)\n",
    "    plot_acf(dataset)\n",
    "    acfval=[0]*len(res)\n",
    "    ub=1.96/np.sqrt(len(dataset))\n",
    "    p1=1\n",
    "    for i in range(len(res)):\n",
    "        acfval[i]=abs(res[i])\n",
    "    acfval.sort(reverse=True)\n",
    "    acfval=np.array(acfval[1:])\n",
    "    pshort=np.array(acfval[0:3])\n",
    "    pshortind=[0]*len(pshort)\n",
    "#    print(pshort)\n",
    "    for i in range(len(pshort)):\n",
    "        pshortind[i]=np.where(abs(res)==pshort[i])[0][0]\n",
    "    ind=np.where(acfval>ub)[0]\n",
    "    finalacf=acfval[ind]\n",
    "    plist=[0]*len(finalacf)\n",
    "    for i in range(len(finalacf)):\n",
    "        plist[i]=np.where(abs(res)==finalacf[i])[0][0]\n",
    "#    return pshortind,plist\n",
    "#    print(plist)\n",
    "    while len(finalacf)>0:\n",
    "        p1=np.where(abs(res)==max(finalacf))[0][0]\n",
    "        if p1 > freq:\n",
    "            finalacf=finalacf[1:]\n",
    "        else:\n",
    "            break\n",
    "    return p1,pshortind,plist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partial autocorrelation measures the relationship \n",
    "#between a variable's current value and its past values separated\n",
    "#by k time units\n",
    "\n",
    "#function for partial autocorrelation\n",
    "def pacf_plot1(dataset,freq):\n",
    "#    plot_pacf(dataset)\n",
    "    res = pacf(dataset)\n",
    "    ub=1.96/np.sqrt(len(dataset))\n",
    "    for i in range(1, len(res)-1):\n",
    "        if(res[i] > ub and res[i + 1] < ub):\n",
    "            q = i\n",
    "            if (q > freq/2):\n",
    "                q = freq/2\n",
    "            break\n",
    "    else:\n",
    "        q = freq/2\n",
    "    return int(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way of plotting pacf\n",
    "def pacf_plot(dataset,freq):\n",
    "    res = pacf(dataset)\n",
    "    plot_pacf(dataset)\n",
    "    pacfval=[0]*len(res)\n",
    "    ub=1.96/np.sqrt(len(dataset))\n",
    "    q1=0\n",
    "    for i in range(len(res)):\n",
    "        pacfval[i]=abs(res[i])\n",
    "    pacfval.sort(reverse=True)\n",
    "    pacfval=np.array(pacfval[1:])\n",
    "    ind=np.where(pacfval>ub)[0]\n",
    "    finalpacf=pacfval[ind]\n",
    "    while len(finalpacf)>0:\n",
    "        q1=np.where(abs(res)==max(finalpacf))[0]\n",
    "        q1=q1[0]\n",
    "        if q1 > int(freq/2):\n",
    "            finalpacf=finalpacf[1:]\n",
    "        else:\n",
    "            break\n",
    "    return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_absolute_deviation(dataset, median):\n",
    "    median_list = list()\n",
    "    dataset.reset_index(drop = True, inplace = True)\n",
    "    for i in range(0, len(dataset)):\n",
    "        value = dataset.T[i] - median\n",
    "        median_list.append(value)\n",
    "    ms = np.abs(median_list)\n",
    "    mad = np.median(ms)\n",
    "    ub = median + (3 * mad)\n",
    "    lb = median - (3 * mad)\n",
    "\n",
    "\n",
    "    return ub,lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateformat(dataset):\n",
    "    dataset['time'] = dataset.index.tolist()\n",
    "    dataset['time']=pd.to_datetime(dataset['time'], format='%m/%d/%y',infer_datetime_format=True)\n",
    "    dataset.set_index('time', inplace = True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_treatment(dataset):\n",
    "    median = np.median(dataset)\n",
    "    if median == 0:\n",
    "        ub,lb = mean_standard_deviation(dataset)\n",
    "    else:\n",
    "        ub,lb = median_absolute_deviation(dataset, median)\n",
    "    new_dataset = np.clip(dataset, lb, ub)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pt--->outlier bucket size\n",
    "#sindex ---> number of zeros to categorize as sparse data\n",
    "#freq ---> seasonality\n",
    "def get_bucket_size(interval):\n",
    "    interval_type = find_interval_type(interval) #aggregation (weekly/monthly)\n",
    "    if interval_type == 'W':\n",
    "        pt=12\n",
    "        sindex=24\n",
    "        freq=52\n",
    "    elif interval_type=='M'or interval_type=='Random':\n",
    "        pt=6\n",
    "        sindex=10\n",
    "        freq=12\n",
    "    elif interval_type=='Y':\n",
    "        pt=2\n",
    "        sindex=0\n",
    "        freq=0\n",
    "    return pt,sindex,freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_treatment_tech(dataset,interval,pt): \n",
    "    start=0\n",
    "    end=pt\n",
    "    sku_data=[0]*len(dataset)\n",
    "    while end < len(dataset):\n",
    "        sku_data[start:end]=outlier_treatment(dataset[start:end])\n",
    "        start=end\n",
    "        end+=pt\n",
    "    if start < len(dataset):\n",
    "        sku_data[start:len(dataset)]=outlier_treatment(dataset[start:end])\n",
    "    sku_data=pd.DataFrame(sku_data)\n",
    "    return sku_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sesonal_detection(sku_data):\n",
    "    median = np.median(sku_data)\n",
    "\n",
    "    if median == 0:\n",
    "        ub,lb = mean_standard_deviation(sku_data)\n",
    "    else:\n",
    "        ub,lb = median_absolute_deviation(sku_data, median)\n",
    "    outliers1= sku_data > ub\n",
    "    outliers2 = sku_data < lb\n",
    "    a=np.where(outliers1==True)[0]\n",
    "    b=np.where(outliers2==True)[0]\n",
    "    flag1=flag2=1\n",
    "    if len(a)==0:\n",
    "        flag1=0\n",
    "        remove1=[]\n",
    "    if len(b)==0:\n",
    "        flag2=0\n",
    "        remove2=[]\n",
    "\n",
    "    if flag1==1:\n",
    "        k=np.zeros([len(a)-1, len(a)])\n",
    "        for i in range(0,(len(a)-1)):\n",
    "            for j in range(1,len(a)):\n",
    "                if a[j]==(a[i]+12) or a[j]==(a[i]+24):\n",
    "                    k[i][j]=1\n",
    "                else:\n",
    "                    k[i][j]=0\n",
    "        m=np.where(k!=0)\n",
    "        z=np.unique(m).tolist()\n",
    "        remove1=a[z]\n",
    "    if flag2==1:\n",
    "        q=np.zeros([len(b)-1, len(b)])\n",
    "        for i in range(0,(len(b)-1)):\n",
    "            for j in range(1,len(b)):\n",
    "                if b[j]==(b[i]+12) or b[j]==(b[i]+24):\n",
    "                    q[i][j]=1\n",
    "                else:\n",
    "                    q[i][j]=0\n",
    "        n=np.where(q!=0)\n",
    "        z1=np.unique(n).tolist()\n",
    "        remove2=b[z1]\n",
    "    return remove1,remove2,flag1,flag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_dates(dataset):\n",
    "    interval,start_date,end_date=find_interval(dataset.index)\n",
    "    drange = pd.date_range(start_date, end_date, freq = interval)\n",
    "    comp_data = pd.DataFrame(drange, columns = ['time'])\n",
    "    sales=dataset['sales'].to_dict()\n",
    "    comp_data['sales'] = comp_data['time'].map(sales)\n",
    "    comp_data.drop('time', axis = 1, inplace = True)\n",
    "    return comp_data, interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_interval(date):\n",
    "    date=pd.to_datetime(date, format='%m/%d/%Y',infer_datetime_format=True)\n",
    "    diff=[]\n",
    "    for i in range(len(date)-1):\n",
    "        interval=date[i+1]-date[i]\n",
    "        diff.append(interval)\n",
    "    mode=statistics.mode(diff)\n",
    "\n",
    "    return mode,date[0],date[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_interval_type(interval):\n",
    "    interval=interval.days\n",
    "    if interval==7:\n",
    "        itype='W'\n",
    "    elif interval==30 or interval==31:\n",
    "        itype='M'\n",
    "    elif interval==365:\n",
    "        itype='Y'\n",
    "    else:\n",
    "        itype='Random'\n",
    "\n",
    "    return itype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_imputation_zero(dataset):\n",
    "    dataset.fillna(0, inplace = True)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Send as transpose of dataset\n",
    "def data_imputation(dataset,freq):\n",
    "    #Taking the mean of nearest neighbours to fill NA\n",
    "\n",
    "    data_forward = dataset.fillna(method = 'ffill')\n",
    "    data_back = dataset.fillna(method = 'bfill')\n",
    "    data_back.fillna(0, inplace = True)\n",
    "    data_forward.fillna(0, inplace = True)\n",
    "\n",
    "    new_data = (data_forward.values + data_back.values) / 2\n",
    "    dataset=pd.DataFrame(dataset.values)\n",
    "\n",
    "    imput=dataset.isnull()\n",
    "    imput=imput[0]\n",
    "\n",
    "    dataset=dataset[0]\n",
    "    for i in range(len(dataset)):\n",
    "        div_factor = 3\n",
    "        if imput[i]==True:\n",
    "            #Negative index, set previous as 0\n",
    "            if i - freq < 0:\n",
    "                prev_value = 0\n",
    "                div_factor -= 1\n",
    "            #Fetch previous value\n",
    "            else:\n",
    "                prev_value = dataset[i - freq]\n",
    "\n",
    "            #Outside boundary or next value is NaN, set previous as 0\n",
    "            if i + freq >= len(dataset) or imput[i + freq]==True:\n",
    "                next_value = 0\n",
    "                div_factor -= 1\n",
    "            else:\n",
    "                next_value = dataset[i + freq]\n",
    "            dataset[i] = (new_data[i] + prev_value + next_value)/div_factor\n",
    "\n",
    "    df = pd.DataFrame(dataset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading from first non-zero\n",
    "def read_from_first_sales(sku_data):\n",
    "    test=pd.isnull(sku_data)\n",
    "    index=np.where(test==False)[0]\n",
    "    index=index[0]\n",
    "    sku_data = sku_data[index:]\n",
    "    sku_data=sku_data.reset_index(drop = True)\n",
    "    return sku_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_forecast(data,forecast,forecast_period):\n",
    "\n",
    "    sample_data=data[-forecast_period:]\n",
    "    sample_data=pd.DataFrame(sample_data)\n",
    "    forecast=pd.DataFrame(forecast)\n",
    "    median_data=np.median(sample_data)\n",
    "    median_fore=np.median(forecast)\n",
    "    if median_data == 0:\n",
    "        ub_d,lb_d = mean_standard_deviation(sample_data)\n",
    "    else:\n",
    "        ub_d,lb_d = median_absolute_deviation(sample_data, median_data)\n",
    "    if median_fore == 0:\n",
    "        ub_f,lb_f = mean_standard_deviation(forecast)\n",
    "    else:\n",
    "        ub_f,lb_f = median_absolute_deviation(forecast, median_fore)\n",
    "    if ub_f >ub_d:\n",
    "        ub=ub_d\n",
    "    else:\n",
    "        ub=ub_f\n",
    "    if lb_f >lb_d:\n",
    "        lb=lb_d\n",
    "    else:\n",
    "        lb=lb_f\n",
    "    forecast=np.clip(forecast,lb,ub)\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function used to fit the model\n",
    "'''\n",
    "\n",
    "def fit_model(train_data, model):\n",
    "\n",
    "    X, y = train_data[:, 0:-1], train_data[:, -1]\n",
    "    model.fit(X, y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function used for one-step forecasting\n",
    "'''\n",
    "\n",
    "def forecast_model(model, X):\n",
    "\n",
    "    yhat = model.predict(X)\n",
    "\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function used for plotting\n",
    "\n",
    "'''\n",
    "def plotting(key, predictions, expected):\n",
    "    rmse = calculate_rmse(key, expected, predictions)\n",
    "    plt.figure()\n",
    "    plt.title(key)\n",
    "\n",
    "    y_values = list(expected) + list(predictions)\n",
    "    y_range = max(y_values) - min(y_values)\n",
    "    plt.text(6, min(y_values) + 0.2 * y_range, 'RMSE = ' + str(rmse))\n",
    "    plt.plot(predictions)\n",
    "    plt.plot(expected)\n",
    "    plt.legend(['predicted', 'expected'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mape = sum(np.abs((y_true - y_pred) / y_true)* 100)/len(y_true)\n",
    "    if np.isnan(mape)== True or np.isfinite(mape)==False:\n",
    "        mape=0\n",
    "    return mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_facc(y_true, y_pred):\n",
    "\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    facc = 1 - (sum(np.abs(y_true - y_pred)) / sum(y_true))\n",
    "    if np.isnan(facc)== True or np.isfinite(facc)==False:\n",
    "        facc=0\n",
    "    return facc * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(key, expected, predictions):\n",
    "    expected=np.array(expected)\n",
    "    rmse = sqrt(mean_squared_error(expected, predictions))\n",
    "    print(\"RMSE FOR %s: %d \" % (key, rmse))\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(test,n,n1):\n",
    "    train = []\n",
    "    train = [x for x in test]\n",
    "    pred = []\n",
    "    for num in range(n):\n",
    "        test_new = pd.DataFrame(train)\n",
    "        pred1 = (test_new.tail(n1).mean())\n",
    "        pred1 = pred1[0]\n",
    "        pred.append(pred1)\n",
    "        train.append(pred1)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_moving_average(test1,n,n1):\n",
    "    alpha=[0.25,0.3,0.45]\n",
    "    train = [x for x in test1]\n",
    "    pred = []\n",
    "    for num in range(n):\n",
    "        test_new = pd.DataFrame(train)\n",
    "        pred1 = test_new.tail(n1)\n",
    "        pred1=np.dot(pred1[0],alpha)\n",
    "        pred.append(pred1)\n",
    "        train.append(pred1)\n",
    "    pred = [int(i) for i in pred]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_repetition(arr, limit, index_start, index_end):\n",
    "    length = index_start\n",
    "    try:\n",
    "        for i in range(0, int( len(arr)/length)):\n",
    "            condition  = np.array( arr[i:int(i+length)]) - np.array( arr[int( i+length):int(i+2*length)])\n",
    "            condition  = np.sum([abs(number) for number in condition])\n",
    "            if condition >= limit :\n",
    "                if length + 1 <= index_end:\n",
    "                    return check_repetition(arr, limit, length + 1, index_end)\n",
    "            # if not than no more computations needed\n",
    "                else:\n",
    "                    return 0\n",
    "\n",
    "            if i == int( len(arr)/length)-2:\n",
    "                return(length)\n",
    "    except:\n",
    "        for i in range(0, int( len(arr)/length)):\n",
    "            if  i+2*length+1 <= index_end and i+length+1 <= index_end:\n",
    "                break\n",
    "            condition  = np.array( arr[i:int(i+length)]) - np.array( arr[int( i+length):int(i+2*length)])\n",
    "            condition  = np.sum([abs(number) for number in condition])\n",
    "            if condition >= limit :\n",
    "                if length + 1 <= index_end:\n",
    "                    return check_repetition(arr, limit, length + 1, index_end)\n",
    "            # if not than no more computations needed\n",
    "                else:\n",
    "                    return 0\n",
    "\n",
    "            if i == int( len(arr)/length)-2:\n",
    "                return(length)\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-423ae5c85bf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'end' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-c5dfc92e54f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Runtime of the program is {end - start}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'end' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Runtime of the program is {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
